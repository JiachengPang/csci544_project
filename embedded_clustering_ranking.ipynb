{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import BertModel, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_embeddings = True\n",
    "load_models = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'validation', 'train'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"stanfordnlp/snli\")\n",
    "dataset.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]\n",
    "validation = dataset[\"validation\"]\n",
    "test = dataset[\"test\"]\n",
    "\n",
    "train = train.filter(lambda x: x[\"label\"] != -1)\n",
    "validation = validation.filter(lambda x: x[\"label\"] != -1)\n",
    "test = test.filter(lambda x: x[\"label\"] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entailment_ids = np.where(np.array(test[\"label\"]) == 0)[0]\n",
    "test_neutral_ids = np.where(np.array(test[\"label\"]) == 1)[0]\n",
    "test_contradiction_ids = np.where(np.array(test[\"label\"]) == 2)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549367, 9842, 9824)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "len(train), len(validation), len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emb(dataset, _batch_size=batch_size):\n",
    "    texts = [data[\"premise\"] + \" \" + data[\"hypothesis\"] for data in dataset]\n",
    "    total_samples = len(texts)\n",
    "\n",
    "    result = torch.zeros(total_samples, 768)\n",
    "\n",
    "    for i in range(0, total_samples, _batch_size):\n",
    "        batch_texts = texts[i : i + _batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "\n",
    "        result[i : i + len(batch_texts), :] = cls_embeddings\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "if load_embeddings:\n",
    "    # test_entailment_emb = torch.from_numpy(np.load(\"test_entailment_emb.npy\"))\n",
    "    # test_neutral_emb = torch.from_numpy(np.load(\"test_neutral_emb.npy\"))\n",
    "    # test_contradiction_emb = torch.from_numpy(np.load(\"test_contradiction_emb.npy\"))\n",
    "    validation_emb = torch.from_numpy(np.load(\"validation_emb.npy\"))\n",
    "    train_emb = torch.from_numpy(np.load(\"train_emb.npy\"))\n",
    "    test_emb = torch.from_numpy(np.load(\"test_emb.npy\"))\n",
    "else:\n",
    "    # test_entailment_emb = extract_emb(test_entailment)\n",
    "    # test_neutral_emb = extract_emb(test_neutral)\n",
    "    # test_contradiction_emb = extract_emb(test_contradiction)\n",
    "\n",
    "    validation_emb = extract_emb(validation)\n",
    "    train_emb = extract_emb(train)\n",
    "    test_emb = extract_emb(test)\n",
    "\n",
    "    # Save embeddings\n",
    "    # np.save(\"test_entailment_emb.npy\", test_entailment_emb.numpy())\n",
    "    # np.save(\"test_neutral_emb.npy\", test_neutral_emb.numpy())\n",
    "    # np.save(\"test_contradiction_emb.npy\", test_contradiction_emb.numpy())\n",
    "    np.save(\"validation_emb.npy\", validation_emb.numpy())\n",
    "    np.save(\"train_emb.npy\", train_emb.numpy())\n",
    "    np.save(\"test_emb.npy\", test_emb.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)\n",
    "\n",
    "# test_entailment_emb_pca = pca.fit_transform(test_entailment_emb)\n",
    "# # print(sum(pca.explained_variance_ratio_))\n",
    "# test_neutral_emb_pca = pca.fit_transform(test_neutral_emb)\n",
    "# # print(sum(pca.explained_variance_ratio_))\n",
    "# test_contradiction_emb_pca = pca.fit_transform(test_contradiction_emb)\n",
    "# # print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "validation_emb_pca = pca.fit_transform(validation_emb)\n",
    "# print(sum(pca.explained_variance_ratio_))\n",
    "train_emb_pca = pca.fit_transform(train_emb)\n",
    "test_emb_pca = pca.fit_transform(test_emb)\n",
    "\n",
    "\n",
    "test_entailment_emb_pca = test_emb_pca[test_entailment_ids]\n",
    "test_neutral_emb_pca = test_emb_pca[test_neutral_ids]\n",
    "test_contradiction_emb_pca = test_emb_pca[test_contradiction_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3368, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_entailment_emb_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/mff7k5qs45x_wns7wf7_b9q00000gn/T/ipykernel_17454/26246381.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  models[i] = torch.load(f\"model_{i}.pt\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "n_models = 5\n",
    "\n",
    "models = {}\n",
    "\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.gru = torch.nn.GRU(64, 64, 10, bidirectional=True)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.fc1 = torch.nn.Linear(128, 64)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if load_models:\n",
    "    for i in range(n_models):\n",
    "        models[i] = torch.load(f\"model_{i}.pt\", map_location=device)\n",
    "else:\n",
    "    for i in range(n_models):\n",
    "        model_ = Classifier()\n",
    "        model_.to(device)\n",
    "        optimizer = torch.optim.AdamW(model_.parameters(), lr=1e-5)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_accuracy = 0\n",
    "        best_loss = 10000\n",
    "\n",
    "        early_stop_cnt = 0\n",
    "        prev_acc = 0\n",
    "\n",
    "        for epoch in range(100):\n",
    "            train_loss = 0\n",
    "            eval_loss = 0\n",
    "            model_.train()\n",
    "            for j in range(0, len(train_emb_pca), batch_size):\n",
    "                batch = train_emb_pca[j : j + batch_size]\n",
    "                batch = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "                labels = torch.tensor(train[j : j + batch_size][\"label\"]).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model_(batch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            print(\n",
    "                f\"Model: {i + 1} *** Epoch {epoch} *** Loss: {train_loss / (len(train_emb_pca) / batch_size)}\"\n",
    "            )\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for j in range(0, len(validation_emb_pca), batch_size):\n",
    "                    batch = validation_emb_pca[j : j + batch_size]\n",
    "                    batch = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "                    labels = torch.tensor(validation[j : j + batch_size][\"label\"]).to(\n",
    "                        device\n",
    "                    )\n",
    "\n",
    "                    output = model_(batch)\n",
    "                    loss = criterion(output, labels)\n",
    "                    eval_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    # print(predicted)\n",
    "                    # print(output.shape)\n",
    "                    # print(np.argmax(predicted.cpu().detach().numpy(), axis=1).shape)\n",
    "                    correct += (\n",
    "                        (\n",
    "                            predicted.cpu().detach().numpy()\n",
    "                            == labels.cpu().detach().numpy()\n",
    "                        )\n",
    "                        .sum()\n",
    "                        .item()\n",
    "                    )\n",
    "\n",
    "            # if eval_loss / (len(validation_emb_pca) / batch_size) < best_loss:\n",
    "            #     best_loss = eval_loss / (len(validation_emb_pca) / batch_size)\n",
    "            #     models[i] = model_.state_dict()\n",
    "            acc = 100 * correct / total\n",
    "            if acc <= prev_acc:\n",
    "                early_stop_cnt += 1\n",
    "            else:\n",
    "                prev_acc = acc\n",
    "                early_stop_cnt = 0\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                models[i] = model_.state_dict()\n",
    "\n",
    "            print(\n",
    "                f\"Model: {i + 1} *** Epoch {epoch} *** Eval Loss: {eval_loss / (len(validation_emb_pca) / batch_size)}\"\n",
    "            )\n",
    "            print(f\"Accuracy: {100 * correct / total:.4f}\")\n",
    "            if early_stop_cnt >= 5:\n",
    "                break\n",
    "        print(f\"Best acc: {best_accuracy}\")\n",
    "\n",
    "    for i in range(n_models):\n",
    "        torch.save(models[i], f\"model_{i}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = np.zeros((len(test), n_models))\n",
    "\n",
    "\n",
    "for i in range(n_models):\n",
    "    model_ = Classifier()\n",
    "    model_.load_state_dict(models[i])\n",
    "    model_.to(device)\n",
    "    model_.eval()\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(test), batch_size):\n",
    "            batch = test_emb_pca[j : j + batch_size]\n",
    "            batch = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "\n",
    "            labels = torch.tensor(test[j : j + batch_size][\"label\"])\n",
    "\n",
    "            output = torch.nn.functional.softmax(model_(batch), dim=1)\n",
    "\n",
    "            num_out = output.shape[0]\n",
    "\n",
    "            row_indices = torch.arange(num_out)\n",
    "\n",
    "            confidence[j : j + num_out, i] = (\n",
    "                output[row_indices, labels[:num_out]].cpu().detach().numpy()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_clusters = int(\n",
    "#     sum((len(test_entailment_ids), len(test_neutral_ids), len(test_contradiction_ids)))\n",
    "#     * 0.02\n",
    "#     // (2 * 3 - 1)\n",
    "# )\n",
    "k_clusters = 40\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_clusters, random_state=42, n_init=\"auto\")\n",
    "\n",
    "test_entailment_labels = kmeans.fit_predict(test_entailment_emb_pca)\n",
    "test_neutral_labels = kmeans.fit_predict(test_neutral_emb_pca)\n",
    "test_contradiction_labels = kmeans.fit_predict(test_contradiction_emb_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entailment_cnt = np.zeros(k_clusters)\n",
    "test_neutral_cnt = np.zeros(k_clusters)\n",
    "test_contradiction_cnt = np.zeros(k_clusters)\n",
    "\n",
    "for i in range(k_clusters):\n",
    "    test_entailment_cnt[i] = np.sum(test_entailment_labels == i)\n",
    "    test_neutral_cnt[i] = np.sum(test_neutral_labels == i)\n",
    "    test_contradiction_cnt[i] = np.sum(test_contradiction_labels == i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = {\n",
    "    \"entailment\": {},\n",
    "    \"neutral\": {},\n",
    "    \"contradiction\": {},\n",
    "}\n",
    "\n",
    "for i in range(k_clusters):\n",
    "    rankings[\"entailment\"][i] = {}\n",
    "    rankings[\"neutral\"][i] = {}\n",
    "    rankings[\"contradiction\"][i] = {}\n",
    "\n",
    "    for j in range(n_models):\n",
    "        temp_confidence = confidence[\n",
    "            test_entailment_ids[test_entailment_labels == i], j\n",
    "        ]\n",
    "        temp_rank_id = np.argsort(-temp_confidence)\n",
    "        temp_rank = temp_rank_id.argsort()\n",
    "        rankings[\"entailment\"][i][j] = temp_rank\n",
    "\n",
    "        temp_confidence = confidence[test_neutral_ids[test_neutral_labels == i], j]\n",
    "        temp_rank_id = np.argsort(-temp_confidence)\n",
    "        temp_rank = temp_rank_id.argsort()\n",
    "        rankings[\"neutral\"][i][j] = temp_rank\n",
    "\n",
    "        temp_confidence = confidence[\n",
    "            test_contradiction_ids[test_contradiction_labels == i], j\n",
    "        ]\n",
    "        temp_rank_id = np.argsort(-temp_confidence)\n",
    "        temp_rank = temp_rank_id.argsort()\n",
    "        rankings[\"contradiction\"][i][j] = temp_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"entailment\": {},\n",
    "    \"neutral\": {},\n",
    "    \"contradiction\": {},\n",
    "}\n",
    "\n",
    "for i in range(k_clusters):\n",
    "    scores[\"entailment\"][i] = test_entailment_cnt[i] * n_models\n",
    "    scores[\"neutral\"][i] = test_neutral_cnt[i] * n_models\n",
    "    scores[\"contradiction\"][i] = test_contradiction_cnt[i] * n_models\n",
    "    for j in range(n_models):\n",
    "        scores[\"entailment\"][i] -= rankings[\"entailment\"][i][j]\n",
    "        scores[\"neutral\"][i] -= rankings[\"neutral\"][i][j]\n",
    "        scores[\"contradiction\"][i] -= rankings[\"contradiction\"][i][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([506., 356., 402., 209., 237., 173., 194., 352., 328.,  32., 446.,\n",
       "       332., 297., 367., 295., 336., 222., 483., 277.,  31., 139., 138.,\n",
       "       520., 181.,  95., 164., 348.,  66., 435., 193., 245., 429., 172.,\n",
       "       380., 204., 228., 101., 454., 266., 101., 472., 313., 219., 238.,\n",
       "       378., 444., 424., 115., 161., 397., 189., 517.,  34.,  61., 421.,\n",
       "       166., 303., 191.,  92.,  36., 325., 201., 474., 447.,  92., 236.,\n",
       "        95., 285., 254., 259., 104., 262., 370.,  48., 268.,  42., 497.,\n",
       "       414., 500., 184., 367., 295., 130., 515., 394., 337.,  17., 196.,\n",
       "        41., 403., 149.,  16., 239., 470., 280., 326., 418.,  92., 201.,\n",
       "       506., 149., 384., 360.,  82., 216., 510.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scores[\"entailment\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24, 174, 128, 321, 293, 357, 336, 178, 202, 498,  84, 198, 233,\n",
       "       163, 235, 194, 308,  47, 253, 499, 391, 392,  10, 349, 435, 366,\n",
       "       182, 464,  95, 337, 285, 101, 358, 150, 326, 302, 429,  76, 264,\n",
       "       429,  58, 217, 311, 292, 152,  86, 106, 415, 369, 133, 341,  13,\n",
       "       496, 469, 109, 364, 227, 339, 438, 494, 205, 329,  56,  83, 438,\n",
       "       294, 435, 245, 276, 271, 426, 268, 160, 482, 262, 488,  33, 116,\n",
       "        30, 346, 163, 235, 400,  15, 136, 193, 513, 334, 489, 127, 381,\n",
       "       514, 291,  60, 250, 204, 112, 438, 329,  24, 381, 146, 170, 448,\n",
       "       314,  20])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    rankings[\"entailment\"][0][0]\n",
    "    + rankings[\"entailment\"][0][1]\n",
    "    + rankings[\"entailment\"][0][2]\n",
    "    + rankings[\"entailment\"][0][3]\n",
    "    + rankings[\"entailment\"][0][4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scores.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"entailment\", \"neutral\", \"contradiction\"])\n",
    "    for i in range(k_clusters):\n",
    "        writer.writerow(\n",
    "            [scores[\"entailment\"][i], scores[\"neutral\"][i], scores[\"contradiction\"][i]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_indices.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"entailment\", \"neutral\", \"contradiction\"])\n",
    "    for i in range(k_clusters):\n",
    "        writer.writerow(\n",
    "            [\n",
    "                test_entailment_ids[test_entailment_labels == i],\n",
    "                test_neutral_ids[test_neutral_labels == i],\n",
    "                test_contradiction_ids[test_contradiction_labels == i],\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save confidence in csv\n",
    "\n",
    "with open(\"confidence.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([f\"model_{i}\" for i in range(n_models)])\n",
    "    for i in range(len(test)):\n",
    "        writer.writerow(confidence[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rankings in csv\n",
    "\n",
    "with open(\"rankings.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"entailment\", \"neutral\", \"contradiction\"])\n",
    "    for i in range(k_clusters):\n",
    "        r_e = 0\n",
    "        r_n = 0\n",
    "        r_c = 0\n",
    "\n",
    "        for j in range(n_models):\n",
    "            r_e += rankings[\"entailment\"][i][j]\n",
    "            r_n += rankings[\"neutral\"][i][j]\n",
    "            r_c += rankings[\"contradiction\"][i][j]\n",
    "\n",
    "        writer.writerow(\n",
    "            [\n",
    "                r_e,\n",
    "                r_n,\n",
    "                r_c,\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cluster indices and scores using pandas\n",
    "\n",
    "scores_df = pd.read_csv(\"scores.csv\")\n",
    "cluster_indices_df = pd.read_csv(\"cluster_indices.csv\")\n",
    "confidence_df = pd.read_csv(\"confidence.csv\")\n",
    "rankings_df = pd.read_csv(\"rankings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_score_ids = []\n",
    "low_k = 4\n",
    "k_clusters = 40\n",
    "\n",
    "\n",
    "def convert_to_list(x):\n",
    "    #  remove all non-numeric characters, split by space and convert to int\n",
    "    x = re.sub(r\"[^0-9 ]\", \" \", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    x = x.split(\" \")\n",
    "    return np.array(x, dtype=int)\n",
    "\n",
    "\n",
    "# indices = np.argpartition(-cos_sim, top_k)[:top_k]\n",
    "for i in range(k_clusters):\n",
    "    for j in [\"entailment\", \"neutral\", \"contradiction\"]:\n",
    "        scores_i = convert_to_list(scores_df[j][i])\n",
    "        ids_i = convert_to_list(cluster_indices_df[j][i])\n",
    "        rankings_i = convert_to_list(rankings_df[j][i])\n",
    "        # print(len(scores_i), len(ids_i), len(rankings_i))\n",
    "        assert len(scores_i) == len(ids_i)\n",
    "        lowest_k = np.argpartition(-rankings_i, low_k)[:low_k]\n",
    "        low_score_ids.extend(ids_i[lowest_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = pd.read_csv(\"intersection_output_ignore_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_ids = intersection[\"index\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  intersection of low_score_ids and intersection_ids\n",
    "low_score_intersection = list(set(low_score_ids).intersection(intersection_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[397,\n",
       " 1162,\n",
       " 1695,\n",
       " 2043,\n",
       " 2487,\n",
       " 2511,\n",
       " 2971,\n",
       " 4000,\n",
       " 4034,\n",
       " 4773,\n",
       " 5189,\n",
       " 5539,\n",
       " 5991,\n",
       " 7215,\n",
       " 7945,\n",
       " 9490]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(low_score_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(low_score_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.404255319148936"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(low_score_intersection) / len(intersection_ids) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
