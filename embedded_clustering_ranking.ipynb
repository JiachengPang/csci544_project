{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import BertModel, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_embeddings = True\n",
    "load_models = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"stanfordnlp/snli\")\n",
    "dataset.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]\n",
    "validation = dataset[\"validation\"]\n",
    "test = dataset[\"test\"]\n",
    "\n",
    "train = train.filter(lambda x: x[\"label\"] != -1)\n",
    "validation = validation.filter(lambda x: x[\"label\"] != -1)\n",
    "test = test.filter(lambda x: x[\"label\"] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entailment_ids = np.where(np.array(test[\"label\"]) == 0)[0]\n",
    "test_neutral_ids = np.where(np.array(test[\"label\"]) == 1)[0]\n",
    "test_contradiction_ids = np.where(np.array(test[\"label\"]) == 2)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "len(train), len(validation), len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emb(dataset, _batch_size=batch_size):\n",
    "    texts = [data[\"premise\"] + \" \" + data[\"hypothesis\"] for data in dataset]\n",
    "    total_samples = len(texts)\n",
    "\n",
    "    result = torch.zeros(total_samples, 768)\n",
    "\n",
    "    for i in range(0, total_samples, _batch_size):\n",
    "        batch_texts = texts[i : i + _batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "\n",
    "        result[i : i + len(batch_texts), :] = cls_embeddings\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "if load_embeddings:\n",
    "    # test_entailment_emb = torch.from_numpy(np.load(\"test_entailment_emb.npy\"))\n",
    "    # test_neutral_emb = torch.from_numpy(np.load(\"test_neutral_emb.npy\"))\n",
    "    # test_contradiction_emb = torch.from_numpy(np.load(\"test_contradiction_emb.npy\"))\n",
    "    validation_emb = torch.from_numpy(np.load(\"validation_emb.npy\"))\n",
    "    train_emb = torch.from_numpy(np.load(\"train_emb.npy\"))\n",
    "    test_emb = torch.from_numpy(np.load(\"test_emb.npy\"))\n",
    "else:\n",
    "    # test_entailment_emb = extract_emb(test_entailment)\n",
    "    # test_neutral_emb = extract_emb(test_neutral)\n",
    "    # test_contradiction_emb = extract_emb(test_contradiction)\n",
    "\n",
    "    validation_emb = extract_emb(validation)\n",
    "    train_emb = extract_emb(train)\n",
    "    test_emb = extract_emb(test)\n",
    "\n",
    "    # Save embeddings\n",
    "    # np.save(\"test_entailment_emb.npy\", test_entailment_emb.numpy())\n",
    "    # np.save(\"test_neutral_emb.npy\", test_neutral_emb.numpy())\n",
    "    # np.save(\"test_contradiction_emb.npy\", test_contradiction_emb.numpy())\n",
    "    np.save(\"validation_emb.npy\", validation_emb.numpy())\n",
    "    np.save(\"train_emb.npy\", train_emb.numpy())\n",
    "    np.save(\"test_emb.npy\", test_emb.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)\n",
    "\n",
    "# test_entailment_emb_pca = pca.fit_transform(test_entailment_emb)\n",
    "# # print(sum(pca.explained_variance_ratio_))\n",
    "# test_neutral_emb_pca = pca.fit_transform(test_neutral_emb)\n",
    "# # print(sum(pca.explained_variance_ratio_))\n",
    "# test_contradiction_emb_pca = pca.fit_transform(test_contradiction_emb)\n",
    "# # print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "validation_emb_pca = pca.fit_transform(validation_emb)\n",
    "# print(sum(pca.explained_variance_ratio_))\n",
    "train_emb_pca = pca.fit_transform(train_emb)\n",
    "test_emb_pca = pca.fit_transform(test_emb)\n",
    "\n",
    "\n",
    "test_entailment_emb_pca = test_emb_pca[test_entailment_ids]\n",
    "test_neutral_emb_pca = test_emb_pca[test_neutral_ids]\n",
    "test_contradiction_emb_pca = test_emb_pca[test_contradiction_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entailment_emb_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5\n",
    "\n",
    "models = {}\n",
    "\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.gru = torch.nn.GRU(64, 64, 10, bidirectional=True)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.fc1 = torch.nn.Linear(128, 64)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if load_models:\n",
    "    for i in range(n_models):\n",
    "        models[i] = torch.load(f\"model_{i}.pt\", map_location=device)\n",
    "else:\n",
    "    for i in range(n_models):\n",
    "        model_ = Classifier()\n",
    "        model_.to(device)\n",
    "        optimizer = torch.optim.AdamW(model_.parameters(), lr=1e-5)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        best_accuracy = 0\n",
    "        best_loss = 10000\n",
    "\n",
    "        early_stop_cnt = 0\n",
    "        prev_acc = 0\n",
    "\n",
    "        for epoch in range(100):\n",
    "            train_loss = 0\n",
    "            eval_loss = 0\n",
    "            model_.train()\n",
    "            for j in range(0, len(train_emb_pca), batch_size):\n",
    "                batch = train_emb_pca[j : j + batch_size]\n",
    "                batch = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "                labels = torch.tensor(train[j : j + batch_size][\"label\"]).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model_(batch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            print(\n",
    "                f\"Model: {i + 1} *** Epoch {epoch} *** Loss: {train_loss / (len(train_emb_pca) / batch_size)}\"\n",
    "            )\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for j in range(0, len(validation_emb_pca), batch_size):\n",
    "                    batch = validation_emb_pca[j : j + batch_size]\n",
    "                    batch = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "                    labels = torch.tensor(validation[j : j + batch_size][\"label\"]).to(\n",
    "                        device\n",
    "                    )\n",
    "\n",
    "                    output = model_(batch)\n",
    "                    loss = criterion(output, labels)\n",
    "                    eval_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    # print(predicted)\n",
    "                    # print(output.shape)\n",
    "                    # print(np.argmax(predicted.cpu().detach().numpy(), axis=1).shape)\n",
    "                    correct += (\n",
    "                        (\n",
    "                            predicted.cpu().detach().numpy()\n",
    "                            == labels.cpu().detach().numpy()\n",
    "                        )\n",
    "                        .sum()\n",
    "                        .item()\n",
    "                    )\n",
    "\n",
    "            # if eval_loss / (len(validation_emb_pca) / batch_size) < best_loss:\n",
    "            #     best_loss = eval_loss / (len(validation_emb_pca) / batch_size)\n",
    "            #     models[i] = model_.state_dict()\n",
    "            acc = 100 * correct / total\n",
    "            if acc <= prev_acc:\n",
    "                early_stop_cnt += 1\n",
    "            else:\n",
    "                prev_acc = acc\n",
    "                early_stop_cnt = 0\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                models[i] = model_.state_dict()\n",
    "\n",
    "            print(\n",
    "                f\"Model: {i + 1} *** Epoch {epoch} *** Eval Loss: {eval_loss / (len(validation_emb_pca) / batch_size)}\"\n",
    "            )\n",
    "            print(f\"Accuracy: {100 * correct / total:.4f}\")\n",
    "            if early_stop_cnt >= 5:\n",
    "                break\n",
    "        print(f\"Best acc: {best_accuracy}\")\n",
    "\n",
    "    for i in range(n_models):\n",
    "        torch.save(models[i], f\"model_{i}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = np.zeros((len(test), n_models))\n",
    "\n",
    "\n",
    "for i in range(n_models):\n",
    "    model_ = Classifier()\n",
    "    model_.load_state_dict(models[i])\n",
    "    model_.to(device)\n",
    "    model_.eval()\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(test), batch_size):\n",
    "            batch = test_emb_pca[j : j + batch_size]\n",
    "            batch = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "\n",
    "            labels = torch.tensor(test[j : j + batch_size][\"label\"])\n",
    "\n",
    "            output = torch.nn.functional.softmax(model_(batch), dim=1)\n",
    "\n",
    "            num_out = output.shape[0]\n",
    "\n",
    "            row_indices = torch.arange(num_out)\n",
    "\n",
    "            confidence[j : j + num_out, i] = (\n",
    "                output[row_indices, labels[:num_out]].cpu().detach().numpy()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clusters = int(\n",
    "    sum((len(test_entailment_ids), len(test_entailment_ids), len(test_entailment_ids)))\n",
    "    * 0.02\n",
    "    // (2 * 3 - 1)\n",
    ")\n",
    "\n",
    "kmeans = KMeans(n_clusters=k_clusters, random_state=42, n_init=\"auto\")\n",
    "\n",
    "test_entailment_labels = kmeans.fit_predict(test_entailment_emb_pca)\n",
    "test_neutral_labels = kmeans.fit_predict(test_neutral_emb_pca)\n",
    "test_contradiction_labels = kmeans.fit_predict(test_contradiction_emb_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entailment_cnt = np.zeros(k_clusters)\n",
    "test_neutral_cnt = np.zeros(k_clusters)\n",
    "test_contradiction_cnt = np.zeros(k_clusters)\n",
    "\n",
    "for i in range(k_clusters):\n",
    "    test_entailment_cnt[i] = np.sum(test_entailment_labels == i)\n",
    "    test_neutral_cnt[i] = np.sum(test_neutral_labels == i)\n",
    "    test_contradiction_cnt[i] = np.sum(test_contradiction_labels == i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = {\n",
    "    \"entailment\": {},\n",
    "    \"neutral\": {},\n",
    "    \"contradiction\": {},\n",
    "}\n",
    "\n",
    "for i in range(k_clusters):\n",
    "    rankings[\"entailment\"][i] = {}\n",
    "    rankings[\"neutral\"][i] = {}\n",
    "    rankings[\"contradiction\"][i] = {}\n",
    "\n",
    "    for j in range(n_models):\n",
    "        # print(confidence[np.where(test_entailment_labels == i), j][0])\n",
    "        # print(rankings[\"entailment\"][i][j])\n",
    "        # print(len(test_entailment_ids[test_entailment_labels == i]))\n",
    "        # print(confidence[test_entailment_ids[test_entailment_labels == i], j])\n",
    "        rankings[\"entailment\"][i][j] = confidence[\n",
    "            test_entailment_ids[test_entailment_labels == i], j\n",
    "        ]\n",
    "        rankings[\"entailment\"][i][j] = np.argsort(rankings[\"entailment\"][i][j])\n",
    "\n",
    "        rankings[\"neutral\"][i][j] = confidence[\n",
    "            test_neutral_ids[test_neutral_labels == i], j\n",
    "        ]\n",
    "        rankings[\"neutral\"][i][j] = np.argsort(rankings[\"neutral\"][i][j])\n",
    "\n",
    "        rankings[\"contradiction\"][i][j] = confidence[\n",
    "            test_contradiction_ids[test_contradiction_labels == i], j\n",
    "        ]\n",
    "        rankings[\"contradiction\"][i][j] = np.argsort(rankings[\"contradiction\"][i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"entailment\": {},\n",
    "    \"neutral\": {},\n",
    "    \"contradiction\": {},\n",
    "}\n",
    "\n",
    "for i in range(k_clusters):\n",
    "    scores[\"entailment\"][i] = test_entailment_cnt[i] * n_models\n",
    "    scores[\"neutral\"][i] = test_neutral_cnt[i] * n_models\n",
    "    scores[\"contradiction\"][i] = test_contradiction_cnt[i] * n_models\n",
    "    for j in range(n_models):\n",
    "        scores[\"entailment\"][i] -= rankings[\"entailment\"][i][j]\n",
    "        scores[\"neutral\"][i] -= rankings[\"neutral\"][i][j]\n",
    "        scores[\"contradiction\"][i] -= rankings[\"contradiction\"][i][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([157., 110., 171., 376., 305., 359., 244., 233., 269., 238., 238.,\n",
       "       310., 271., 204., 225., 239., 151., 224., 244., 254., 294., 260.,\n",
       "       179., 242., 301., 226., 340., 205., 320., 290., 364., 132., 249.,\n",
       "       275., 285., 316., 200., 306., 264., 266., 351., 299., 265., 407.,\n",
       "       313., 244., 266., 282., 297., 421., 315., 176., 266., 343., 247.,\n",
       "       292., 304., 286., 225., 186., 243., 323., 233., 256., 374., 187.,\n",
       "       285., 429., 259., 418., 335., 399., 282., 260., 251., 391., 144.,\n",
       "       265., 161., 327.,  96., 157., 338., 274., 244., 225., 227., 439.,\n",
       "       329., 322., 263., 279., 294., 211., 262., 311., 295., 387., 121.,\n",
       "       233., 148., 148., 359., 136., 296., 218.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"entailment\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
